% !TeX spellcheck = en_US
\documentclass{scrartcl}

\input{header}

\title{Super Resolution using Adversarial Network}
\subtitle{Deep Vision Project}
\author{Jonas MÃ¼ller}
\date{\nth{\day} \MONTH, \the\year}

\begin{document}
\maketitle
	\fxnote{remove draft from fixme}

\tableofcontents

\section{Introduction}
\subsection{Problem}
Single image super resolution is the task to create a \enquote{realistic} higher resolution image \( \IHR \) of a given image \( \ILR \).
Because there is not an a priori mapping \( \ILR\mapsto\IHR \), this task is not well defined.\fxnote{?}

To overcome this problem in praxis and generate trainings data a function \( f\colon \IHR \mapsto \ILR \) is defined, and it is assumed that \( f(\IHR) \) is an image that would upscale to \( \IHR \).
Since the dimension of high resolution images is far greater than the dimension of low resolution images this function can not be injective.
So this task is more complex than solving \( \argmin_{\hat I} \lVert f(\hat I) - \ILR \rVert_2 \) (\autoref{fig:resize} shows the result of this optimization).

The function \( f \) is usually defined as \( f(x) = (x * g) \downarrow_{s} \) where \( g \) is some blurring kernel, \( s \) is the scaling factor, and \( (x\downarrow_{s})_{i,j} := x_{si,sj} \) \parencite{survey_sr}.
In this project the used kernel is a Gaussian kernel with standard deviation \(\sigma=1\).
This process is visualized in \autoref{fig:resize}.
\fxnote{Use?}

\begin{figure}
\begin{center}
\begin{subfigure}[t]{.47\textwidth}
	\centering
	\includegraphics[width=0.87\linewidth]{pulse_img.pdf}
	\caption{Multiple natural high resolution images that map to the same low resolution image. Adopted from \textcite{pulse}.}
	\label{fig:pulse_img}
\end{subfigure}\hfill%
\begin{subfigure}[t]{.47\textwidth}
	\centering
	\includestandalone[width=0.9\linewidth]{standalone/resize_process}
	\caption{The process of transforming a high resolution image (top left) to a low resolution image (bottom right) using \(s=4\).
	And another high resolution image (bottom left) generated by \( \argmin_{\hat I} \lVert f(\hat I) - \ILR \rVert_2 \).}
	\label{fig:resize}
\end{subfigure}%
\caption{Effects of scaling and uniqueness}
\label{fig:scaling}
\end{center}
\end{figure}

\subsection{Dataset}

The used dataset is the Flickr-Faces-HQ Dataset by \textcite{style_gan}.
It consists of \num{70000} images of faces under creative commons licenses crawled from \href{https://www.flickr.com}{flickr.com}.
The original images are of size \( \num{1024} \times \num{1024} \).
The high resolution images in this project are to \( \num{256} \times \num{256} \) downscaled versions of these images.
\autoref{fig:ffhq-teaser} shows an sample of these images.

This dataset was then split into \num{55000} training, \num{10000} validation and \num{5000} test images.
The validation data was used to monitor the progress during training and to adjust the hyperparameters.
And the test set was first used when models were fully trained to compare the final performance of different approaches.
\fxnote{Example Images}

\begin{figure}
	\includegraphics[width=\linewidth]{ffhq-teaser}
	\caption{Example images from the FFHQ, graphic adopted from \textcite{style_gan}.}
	\label{fig:ffhq-teaser}
\end{figure} 

\section{Method}

\subsection{Model}
The model is split into four blocks. The whole model can be seen in \autoref{fig:model}, the intention behind the individual blocks is described below.

The first block is composed of three convolutional layers with kernel size 1.
That is inspired by \textcite{color_net}, who found that the color space used as input affects the performance of the model.  
So this \enquote{Color Space Mapping}-block can be seen as a fully connected neural network on each pixel and is intended to learn a mapping to a favorable color space.
If the RGB color space is the most useful one for this problem, this block can easily learn the identity function.

The second block is a standard convolutional network.
It consists of four residual blocks each with two convolutional layers and an additional convolutional layer.
This block learns a complex abstract representation of the image which will be used in the next block for upscaling.
The residual blocks are sensible here for two reasons: in each residual block it learns the changes necessary to the input but does not have to retain the input itself; and the gradient can be carried backwards through many blocks with this approach because the gradients will not vanish.
The model could be improved by increasing the number of residual blocks.

The next block upscales the current feature using transposed convolutional layers.
The filters have a kernel size of \( 3 \) and stride \( 2 \), so each layer doubles the size, therefore \( \log_2(s) \) many layers (light green in \autoref{fig:model}) are needed.
\textcite{sr_transposed} use this technique for upscaling in their model.
Another common way to increase the resolution of the image would be to use bicubic upscaling on \( \ILR \) and insert the upscaled image into the network, effectively learning to increase the sharpness of the image \parencite{srcnn_baseline, survey_sr}.
Using transposed convolutions brings the advantage that the size of the perceptive field on \( \ILR \) is increased, because the upscaling spaces the features of the image out.\fxnote{Grammatik?}
Also the amount of computations in the previous network is decreased because the image is smaller.
A disadvantage with this is that it only allows upscaling by factors of power two.
To solve this the kernel size and stride could be adjusted, e.\,g. for an upscaling factor of \( 3 \) a kernel size of \( 5 \) and stride \( 3 \) would give the desired result.
But this was not tested in this project.

The last block enables the network to only learn the difference between \( \IHR \) and the nearest neighbor upsample of \( \ILR \).
The basic idea here is to feed the input to the result so that the network can easily learn the identity function and then make small changes.
This is done often, e.\,g. by \textcite{vdsr}, usually by element-wise addition, i.\,e. \( T(\ILR) \oplus \mathrm{Up}(\ILR) \).
This block here concatenates the result of the upscaled version and the result of the network and again uses an pixel-wise fully connected network, to learn a more complex combination than addition.


\begin{figure}
\begin{center}
	\includestandalone[width=\linewidth]{standalone/model}
	\caption{Each convolutional layer has a padding such that the size stays the same, the format is \enquote{Conv: \(k, f\)} where \(k\) is the kernel size and \(f\) the number of filters.
	The upscaling factor \(s\) has to be a power of two.
	The layers that are not specified have the same hyperparameters as the previous layer of the same type (type is indicated by color). And \( \doubleplus \) denotes the concatenation of both inputs.}
	\label{fig:model}
\end{center}
\end{figure}

\subsection{Perceptual Loss}

Perceptual loss was proposed by \textcite{perceptual_loss}.
The idea is to compare \( T(\ILR) \) and \( \IHR \) by their high level features and not by their pixel-wise difference.
For this an auxiliary network \( \phi \) is used, e.\,g. the VGG\textsubscript{16} network from \textcite{vgg} (see \autoref{fig:vgg_perceptual}).
For a given layer \( \ell \), let \( \phi^\ell \) be the function that evaluates \( \phi \) up to \( \ell \).
We can then define the perceptual loss by
\[
	L_{\phi, \ell}(x, y) := \frac{1}{N} \lVert \phi^\ell(x)  - \phi^\ell(y) \rVert^2_2
	\,,
\]
where \( N \) is the number of outputs neurons of \( \ell \).

An idea that I wanted to try out was to use an discriminator network, that distinguishes upscaled from high resolution images, as the auxiliary network, a concept similar to a GAN \parencite{gan_goodfellow}.
The plan was that the discriminator would detect features that the model does not synthesize well and the model could then improve specifically these problems.


\begin{figure}
\begin{center}
	\includestandalone[width=.8\linewidth]{standalone/vgg_perception}
	\caption{Part of the VGG\textsubscript{16} network.
		The proposed extraction layer for super resolution by \textcite{perceptual_loss} is \( \phi_{\mathrm{VGG_{16}}}^{2,2} \).}
	\label{fig:vgg_perceptual}
\end{center}
\end{figure}

\subsection{Regularization}

When using perceptual loss it is necessary to use regularization \parencite{perceptual_loss}.
Because optimizing through a convolutional layer will result in noisy images and regularization smooths the result, as was discussed on exercise sheet 9.

So by using regularization the result is pushed into the direction of the natural image manifold.
\textcite{srgan} proposes to train a discriminator, and use its loss as regularization to push the image in the direction of the manifold of trainings images.

\subsection{Discriminator}

The discriminator used in this project is a less deep variant of the discriminator used by \textcite{stylegan0}.
It is trained to distinguish upscaled images from original high resolution images.
For this six convolutional blocks followed by a shallow fully connected network are used, (\autoref{fig:discriminator}).
Each convolutional block has two convolutional layers and a max pooling layer, so it halves the size of the input.

\begin{figure}
	\begin{center}
		\includestandalone[width=\linewidth]{standalone/disc}
		\caption{The discriminator}
		\label{fig:discriminator}
	\end{center}
\end{figure}

\section{Results}

\subsection{Evaluation Criteria}

Measuring the quality of upscaled images faces similar challenges as defining a loss function.
Therefore the most robust way for evaluation would be to asses the quality by humans, and in this report there is an emphasis on the visual results.
However there are also two commonly used metrics for automated evaluation, which will also be used here, because it is not viable to evaluate every image separately.

There is the peak signal-to-noise-ratio (PSNR) which is based on the mean squared error (MSE).
Because of this a higher PSNR does not necessarily imply a better visual quality.
It is defined by
\[
	\mathrm{PSNR}(I^{\mathrm{orig}}, I^{\mathrm{approx}})
	= 10\cdot \log_{10}\Big(\frac{L^2}{\mathrm{MSE}(I^{\mathrm{orig}}, I^{\mathrm{approx}})}\Big)
	\,,
\]
where
\[
	L = \max I^{\mathrm{orig}}\,,
	\qquad
	\mathrm{MSE}(I^{\mathrm{orig}}, I^{\mathrm{approx}})
	= \frac{1}{3\cdot W \cdot H} \sum_{c=1}^3\sum_{n=1}^W\sum_{m=1}^H \Big(I^{\mathrm{orig}}_{cnm} - I^{\mathrm{approx}}_{cnm}\Big)^2
	\,.
\]

To improve on this problems the Structural Similarity Index Measure (SSIM) was designed:
\[
	\mathrm{SSIM}(I, J)
	= \frac{2\mu_I\mu_J + k_1}{\mu_I^2+\mu_J^2+k_1} \cdot \frac{\sigma_{IJ} + k_2}{\sigma_I^2 + \sigma_J^2 + k_2}
	\,,
\]
where \( \mu_I \) is the mean of \( I \), \( \sigma_{I^2} \) is the variance of \( I \), \( \sigma_{IJ} \) is the covariance of \( I \) and \( J \) and \( k_1, k_2 \) are constants.

For both criteria a higher score is a better score.

\subsection{Doubled Resolution}

Upscaling images by factor of two achieved visually pleasing results.



\begin{figure}
	\begin{center}
		\includestandalone[width=\linewidth]{standalone/2x_upscaling_single}
		\caption{Upscaling results of differently trained models.}
		\label{fig:2x_upscaling_single}
	\end{center}
\end{figure}

\subsection{2x nur krasser}

\section{Conclusion and Future Work}

\pagebreak
\nocite{*}
\printbibliography
\end{document}