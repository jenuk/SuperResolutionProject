% !TeX spellcheck = en_US
\documentclass{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

\frenchspacing

\usepackage{amsmath, mathtools}
\usepackage{nicefrac}

\usepackage{microtype}
\usepackage[super]{nth}

\usepackage[dvipsnames]{xcolor}

\usepackage[backend=biber, style=authoryear, citestyle=authoryear]{biblatex} 
\addbibresource{main.bib}

\newcommand{\MONTH}{%
	\ifcase\the\month
	\or January% 1
	\or February% 2
	\or March% 3
	\or April% 4
	\or May% 5
	\or June% 6
	\or July% 7
	\or August% 8
	\or September% 9
	\or October% 10
	\or November% 11
	\or December% 12
	\fi}

\author{}
\title{Super Resolution using Adversarial Network}
\subtitle{Project Proposal Deep Vision}
\date{\vspace{-4em}}

\setcounter{tocdepth}{1}

\usepackage[ocgcolorlinks=true, citecolor=Green]{hyperref}

\begin{document}
\maketitle

\paragraph{Team}
Jonas MÃ¼ller

\paragraph{Problem Definition}
The problem I want to model is Single Image Super Resolution (\emph{SISR}).
That means, the input is a single image \(I\) of low resolution \(d_1 \times d_1\) from some specific domain, e.\,g. photographs of faces.
Were it's assumed that \(I\) is a scaled down version of a high resolution \(d_2 \times d_2\) image \(I^*\).

The goal then is to compute an image \(O\) of high resolution $d_2 \times d_2$ that approximates \(I^*\) without having seen $I^*$ or in a case where it does not exist.
See \hyperlink{evaluation}{evaluation section} for the question what ``approximate'' means here.
\smallskip

There are several application one might consider to answer the question why super resolution is interesting.
SISR was recently used by \cite{jing2020feature} as a preprocessor for classification when only low resolution data was available.

Another use case for SR is the upscaling of images or videos for everyday life applications.
E.\,g. incoming low quality web cam feeds in video conferences or when watching old videos/movies.
Another solution for low quality web cam feeds that are caused by low bandwidth would be to use a stronger compressor like an auto-encoder, however this would break for users which have only very limited hardware unlike an SR approach.

\paragraph{Dataset}
Flickr-Faces-HQ Dataset \hfill \url{https://github.com/NVlabs/ffhq-dataset} \\
This dataset includes 70,000 images of faces that were scrapped from the website \url{https://www.flickr.com/}.
The images have a resolution of \(1,024 \times 1,024\) pixels.
The dataset also includes some metadata, e.\,g. the date the photo was originally uploaded, however these have no relevance for this task.
This dataset was originally used to train a GAN, see \cite{karras2019style}.

The dataset is already split into a training and validation part with \(60,000\) and \(10,000\) images respectively. I will also further split images from the trainings set into a test set, for the final evaluation of my model.

The images may need to be scaled down for my hardware, see \hyperlink{hardware}{hardware section}.

I will be using these images as my high resolution image $I^*$ and generate down scaled version $I$ of the images.


\paragraph{Approach}
The core goal is to build a deep convolutional neural network $T_{d_1}$ that takes input images of size $d_1 \times d_1$ and returns an upscaled version of size $d_2 \times d_2$.
This seems like a good starting point as it already worked well in \cite{dong2015image}.

\cite{johnson2016perceptual} achieve good results using the high level feature extraction from a related neural network.
So I want to try to train a discriminator network to distinguish upscaled images and original high resolution images; and then use its high level features to  calculate the loss between the upscaled image \(O\) and the original image \(I^*\).
This approach seems appropriate because such a discriminator should learn the most important features to distinguish real from upscaled images. 


\paragraph{Evaluation}\hypertarget{Evaluation}
I want to use the Peak Signal-to-Noise Ratio (PSNR) to evaluate the quality of the reconstructed images, this is a metric that is supposed to model human perception of restored images quality. This was also used by \cite{dong2015image}, and \cite{johnson2016perceptual}.
The results of PSNR seem to depend on the specific test data, so I do not know how to give an explicit performance before starting the project.
My expected result is that the model will outperform classic, non-deep models, so a way to evaluate the network would be to find an implementation or implement such a model myself and compare it on the specific test set.

The most important result is human perception of the result, whether it looks real or not.
To answer this without doing a questionnaire, the results of the trained discriminator may be used as an indicator of this measure.
Again I do not know what ratio of upscaled images can be expected to be misclassified as real.
\smallskip

Furthermore I want to compare the results for different low resolutions \(d_1\) and answer the question what fraction \(\nicefrac{d_2}{d_1}\) is sufficient to produce a coherent output image.
My expectation would be that \(4d_1 \geq d_2\) will be sufficient for results that beat classic methods.


\paragraph{Hardware}\hypertarget{hardware}
I will either use the hardware provided by google colabs or the hardware provided by you.
I am not sure how demanding this project is for different values of \(d_1, d_2\), but I think I will need a smaller \(d_2\) then the provided \(1,024\).
However, this project should very scalable to hardware performance by scaling \(d_1, d_2\) appropriately.

\paragraph{Excluded Presentation Date} None.

\printbibliography

\end{document}